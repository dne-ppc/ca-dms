# Production Docker Compose Override
# Usage: docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d

version: '3.8'

services:
  postgres:
    environment:
      # Use stronger settings for production
      - POSTGRES_INITDB_ARGS=--auth-local=md5 --auth-host=md5
    volumes:
      # Use named volume for better performance
      - postgres_prod_data:/var/lib/postgresql/data
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'
    restart: always
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  redis:
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'
    restart: always
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  backend:
    environment:
      # Production environment variables
      - ENVIRONMENT=production
      - DEBUG=false
      - LOG_LEVEL=INFO
      # Use production database URL with connection pooling
      - DATABASE_URL=${DATABASE_URL}?pool_size=20&max_overflow=30
      # Enable production optimizations
      - WORKERS=4
      - WORKER_CONNECTIONS=1000
      - KEEPALIVE=2
      - MAX_REQUESTS=1000
      - MAX_REQUESTS_JITTER=100
      # Security settings
      - SECURE_SSL_REDIRECT=true
      - SECURE_PROXY_SSL_HEADER=HTTP_X_FORWARDED_PROTO,https
      - SECURE_BROWSER_XSS_FILTER=true
      - SECURE_CONTENT_TYPE_NOSNIFF=true
      - SECURE_FRAME_DENY=true
    command: >
      sh -c "
        python -m alembic upgrade head &&
        gunicorn app.main:app 
        --bind 0.0.0.0:8000
        --workers 4
        --worker-class uvicorn.workers.UvicornWorker
        --worker-connections 1000
        --max-requests 1000
        --max-requests-jitter 100
        --keepalive 2
        --timeout 30
        --graceful-timeout 30
        --access-logfile -
        --error-logfile -
        --log-level info
      "
    deploy:
      replicas: 2
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
    restart: always
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  frontend:
    environment:
      # Production frontend settings
      - NODE_ENV=production
      - VITE_API_BASE_URL=${PRODUCTION_BACKEND_URL}
      - VITE_WS_BASE_URL=${PRODUCTION_WS_URL}
    deploy:
      replicas: 2
      resources:
        limits:
          memory: 256M
          cpus: '0.5'
        reservations:
          memory: 128M
          cpus: '0.25'
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
    restart: always
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Production nginx load balancer
  nginx:
    image: nginx:alpine
    profiles: [] # Remove from profiles to enable by default
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - nginx_cache:/var/cache/nginx
    environment:
      - NGINX_WORKER_PROCESSES=auto
      - NGINX_WORKER_CONNECTIONS=1024
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.5'
        reservations:
          memory: 128M
          cpus: '0.25'
    restart: always
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Log aggregation (optional)
  fluentd:
    image: fluentd:v1.16-alpine-1
    volumes:
      - ./fluentd:/fluentd/etc
      - /var/log:/var/log:ro
    environment:
      FLUENTD_CONF: fluent.conf
    profiles:
      - logging
    restart: unless-stopped

  # Monitoring (optional)
  prometheus:
    image: prom/prometheus:latest
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    volumes:
      - ./prometheus:/etc/prometheus
      - prometheus_data:/prometheus
    profiles:
      - monitoring
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana:/etc/grafana/provisioning
    profiles:
      - monitoring
    restart: unless-stopped

volumes:
  postgres_prod_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /data/postgres
  nginx_cache:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local

networks:
  default:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16